#!/usr/bin/env python3
"""
é€‚é… RTX 4090 çš„äººè„¸è¯†åˆ«ç³»ç»Ÿ (å¢å¼ºç‰ˆ)
æ£€æµ‹ï¼šInsightFace (buffalo_l)
è¯†åˆ«ï¼šArcFace (facenet-pytorch)
ç»˜å›¾ï¼šPIL + Noto Serif CJK (æ”¯æŒä¸­æ–‡æ˜¾ç¤º)
"""

import os
import cv2
import numpy as np
import argparse
from pathlib import Path
from tqdm import tqdm

# å…³é”®å¯¼å…¥
from insightface.app import FaceAnalysis
import torch
import torch.nn.functional as F
from facenet_pytorch import InceptionResnetV1
from PIL import Image, ImageDraw, ImageFont, ImageColor
import torchvision.transforms as transforms

class FaceRecognizer:
    def __init__(self, dataset_path, threshold=0.6, arcface_threshold=0.6, device=None):
        self.threshold = threshold
        self.arcface_threshold = arcface_threshold
        
        # 1. è®¾å¤‡åˆå§‹åŒ–
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # 2. å­—ä½“åŠ è½½é€»è¾‘ (Noto Serif CJK Regular)
        # å¸¸è§è·¯å¾„: Ubuntu çš„ opentype æˆ– truetype ç›®å½•
        font_paths = [
            "/usr/share/fonts/opentype/noto/NotoSerifCJK-Regular.ttc",
            "/usr/share/fonts/truetype/noto/NotoSerifCJK-Regular.ttc",
            "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
            "NotoSerifCJK-Regular.ttc" # å¦‚æœä½ åœ¨å½“å‰ç›®å½•æ”¾äº†å­—ä½“æ–‡ä»¶
        ]
        self.font = None
        self.font_size = 24
        for p in font_paths:
            if os.path.exists(p):
                self.font = ImageFont.truetype(p, self.font_size)
                print(f"âœ… æˆåŠŸåŠ è½½å­—ä½“: {p}")
                break
        if self.font is None:
            print("âš ï¸ æœªæ‰¾åˆ° NotoSerif å­—ä½“ï¼Œå°†ä½¿ç”¨ç³»ç»Ÿé»˜è®¤å­—ä½“ï¼ˆå¯èƒ½ä¸æ”¯æŒä¸­æ–‡ï¼‰")
            self.font = ImageFont.load_default()

        # 3. åˆå§‹åŒ– RetinaFace (buffalo_l)
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ– RetinaFace (buffalo_l)...")
        self.app = FaceAnalysis(
            name='buffalo_l', 
            allowed_modules=['detection'], 
            providers=['CUDAExecutionProvider']
        )
        self.app.prepare(ctx_id=0, det_size=(1280, 1280))

        # 4. åˆå§‹åŒ– ArcFace æ¨¡å‹
        print("ğŸ”„ åˆå§‹åŒ– ArcFace æ¨¡å‹...")
        self.arcface_model = InceptionResnetV1(pretrained='vggface2').eval().to(self.device)

        # æ•°æ®é¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((160, 160)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])

        # 5. åŠ è½½æ•°æ®åº“
        self.dataset_path = dataset_path
        self.face_database = {}
        self._load_face_database()

    def _load_face_database(self):
        """åŠ è½½äººè„¸æ•°æ®åº“å¹¶æå–ç‰¹å¾"""
        if not os.path.exists(self.dataset_path):
            print(f"âŒ æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨: {self.dataset_path}")
            return

        person_folders = [f for f in os.listdir(self.dataset_path) 
                         if os.path.isdir(os.path.join(self.dataset_path, f))]
        
        for person_name in tqdm(person_folders, desc="æå–åº“ç‰¹å¾"):
            person_path = os.path.join(self.dataset_path, person_name)
            features = []
            img_list = list(Path(person_path).glob("*.jpg")) + list(Path(person_path).glob("*.png"))
            
            for img_path in img_list[:5]:
                try:
                    img = Image.open(img_path).convert('RGB')
                    img_tensor = self.transform(img).unsqueeze(0).to(self.device)
                    with torch.no_grad():
                        feature = self.arcface_model(img_tensor)
                        feature = F.normalize(feature, p=2, dim=1)
                        features.append(feature.cpu().numpy())
                except Exception as e:
                    print(f"âš ï¸ åº“å›¾ç‰‡æå–å¤±è´¥ {img_path}: {e}")
            
            if features:
                self.face_database[person_name] = {
                    'name': person_name,
                    'features': np.mean(features, axis=0)
                }
        print(f"âœ… æ•°æ®åº“åŠ è½½å®Œæˆ: {len(self.face_database)} äºº")

    def detect_faces(self, image):
        try:
            faces = self.app.get(image)
            detected_faces = []
            for face in faces:
                if face.det_score < self.threshold: continue
                bbox = face.bbox.astype(int)
                x1, y1, x2, y2 = bbox
                h_img, w_img = image.shape[:2]
                x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w_img, x2), min(h_img, y2)
                face_img = image[y1:y2, x1:x2]
                if face_img.size > 0:
                    detected_faces.append({'bbox': (x1, y1, x2, y2), 'face_img': face_img})
            return detected_faces
        except Exception: return []

    def recognize_faces(self, image):
        """ä½¿ç”¨ PIL ç»˜åˆ¶æ–‡æœ¬ä»¥æ”¯æŒä¸­æ–‡å’Œ Noto å­—ä½“"""
        faces = self.detect_faces(image)
        if not faces:
            return [], image

        # è½¬æ¢ä¸º PIL å›¾åƒè¿›è¡Œç»˜åˆ¶
        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        img_pil = Image.fromarray(img_rgb)
        draw = ImageDraw.Draw(img_pil)
        
        results = []
        for i, face_info in enumerate(faces):
            bbox = face_info['bbox']
            face_img = face_info['face_img']
            
            # ç‰¹å¾è¯†åˆ«
            face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
            pil_face = Image.fromarray(face_rgb)
            img_tensor = self.transform(pil_face).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                feature = self.arcface_model(img_tensor)
                feature = F.normalize(feature, p=2, dim=1).cpu().numpy()

            best_match, max_sim = None, 0
            for person_name, info in self.face_database.items():
                sim = np.dot(feature, info['features'].T)[0][0]
                if sim > max_sim:
                    max_sim = sim
                    best_match = person_name if sim > self.arcface_threshold else None

            # ç»˜åˆ¶é€»è¾‘
            x1, y1, x2, y2 = bbox
            # é¢œè‰²å®šä¹‰ (RGB)
            rect_color = (0, 255, 0) if best_match else (255, 0, 0)
            display_name = best_match if best_match else "æœªçŸ¥"
            label = f"{display_name} {max_sim:.2f}"
            
            # ç”»æ¡†
            draw.rectangle([x1, y1, x2, y2], outline=rect_color, width=4)
            
            # ç”»æ–‡å­—èƒŒæ™¯å—
            # ä½¿ç”¨ textbbox è·å–æ–‡å­—èŒƒå›´ (x0, y0, x1, y1)
            text_bbox = draw.textbbox((x1, y1 - self.font_size - 10), label, font=self.font)
            draw.rectangle(text_bbox, fill=rect_color)
            
            # å†™å­—
            draw.text((x1, y1 - self.font_size - 10), label, font=self.font, fill=(255, 255, 255))
            
            results.append({'face_id': i+1, 'matched_person': best_match, 'score': float(max_sim)})

        # è½¬å› BGR ç”¨äº OpenCV ä¿å­˜
        final_img = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
        return results, final_img

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", default="/hdd/f25/zgq/retina_face_test/196")
    parser.add_argument("--dataset", default="/hdd/f25/zgq/retina_face_test/Dataset")
    parser.add_argument("--output", default="recognition_results")
    args = parser.parse_args()

    os.makedirs(args.output, exist_ok=True)
    recognizer = FaceRecognizer(args.dataset)

    image_files = list(Path(args.input).glob("*.jpg")) + list(Path(args.input).glob("*.png"))
    for img_path in tqdm(image_files, desc="å¤„ç†ä¸­"):
        img = cv2.imread(str(img_path))
        if img is None: continue
        _, vis_img = recognizer.recognize_faces(img)
        cv2.imwrite(os.path.join(args.output, f"res_{img_path.name}"), vis_img)
    print(f"ğŸ å®Œæˆï¼ç»“æœå­˜æ”¾åœ¨: {args.output}")

if __name__ == "__main__":
    main()